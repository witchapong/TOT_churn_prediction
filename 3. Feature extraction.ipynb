{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import feather\n",
    "import datetime as dt\n",
    "from IPython.core.display import display\n",
    "pd.options.display.max_columns = 999\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8b906561444f00b14875e494e3e237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month: 1, year: 2017 skipped...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/witchapong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:92: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "IMPORTANT CHANGE FROM ORIGINAL VERSION:\n",
    "-invoice table was change to the asof month, year style\n",
    "-preprocess the data so that the model will aim to predict churn 60 day in advance\n",
    "-elapse date is referenced with month, date of the table\n",
    "-all the data with elapse date <60 days are filtered out\n",
    "-elapse day is adjusted 60 days back (-60) \n",
    "'''\n",
    "\n",
    "# account\n",
    "df_accs = feather.read_dataframe('../src/df_accs_samp-raw')\n",
    "df_accs = df_accs.drop('START_DT',axis=1)\n",
    "df_accs = df_accs.set_index('ACCOUNT')\n",
    "df_accs = df_accs.dropna(subset=['PROVINCE'])\n",
    "cat_cols = ['BILL_DELIVER','PAY_METHOD','PROVINCE']\n",
    "enc_dict = dict()\n",
    "for col in  cat_cols:\n",
    "    enc_dict[col] = LabelEncoder()\n",
    "    df_accs[col] = enc_dict[col].fit_transform(df_accs[col])\n",
    "\n",
    "with open('../out/enc.dictionary', 'wb') as config_dictionary_file: \n",
    "    pickle.dump(enc_dict, config_dictionary_file)\n",
    "    \n",
    "for year, month in tqdm_notebook(zip([2017]*12 + [2018]*12, list(range(1,13))*2)):\n",
    "    \n",
    "    try:\n",
    "        # aggregation of past product\n",
    "        df_prods = pd.read_csv('../bin/monthly/df_product_'+str(year).zfill(4)+str(month).zfill(2)+'.csv')\n",
    "        df_prods = df_prods.loc[(df_prods.elapse_end > 60) | (df_prods.elapse_end == -1)]\n",
    "        \n",
    "        # adjust elapse date\n",
    "        df_prods['elapse_end'] = df_prods['elapse_end'].apply(lambda x:x-60 if x>60 else x)\n",
    "        df_prods['elapse_start'] = df_prods['elapse_start'].apply(lambda x:x-60 if x>60 else x)\n",
    "        df_prods_agg_past = df_prods.loc[df_prods.elapse_end != -1].groupby('ACCOUNT').agg({'TEL':[len,lambda x:x.nunique()],\n",
    "                                                                                   'elapse_start':[np.max,np.min,np.median,np.mean],\n",
    "                                                                                           'elapse_end':[np.max,np.min,np.median,np.mean],\n",
    "                                                                                           'CHARGE':[np.max,np.min,np.median,np.mean]})\n",
    "\n",
    "        df_prods_agg_past.columns = [col + '-' + stat for col, stat in zip(df_prods_agg_past.columns.get_level_values(0),df_prods_agg_past.columns.get_level_values(1))]\n",
    "\n",
    "        # aggregation of active product\n",
    "        df_prod_agg_act = df_prods.loc[df_prods.elapse_end == -1].groupby('ACCOUNT').agg({'TEL':[len,lambda x:x.nunique()],\n",
    "                                                                                   'elapse_start':[np.max,np.min,np.median,np.mean],\n",
    "                                                                                           'elapse_end':[np.max,np.min,np.median,np.mean],\n",
    "                                                                                           'CHARGE':[np.max,np.min,np.median,np.mean]})\n",
    "        df_prod_agg_act.columns = [col + '-' + stat for col, stat in zip(df_prod_agg_act.columns.get_level_values(0),df_prod_agg_act.columns.get_level_values(1))]\n",
    "\n",
    "        acc_list = df_prods.ACCOUNT.unique()\n",
    "        tel_n_acc = df_prods[['ACCOUNT','TEL']]\n",
    "\n",
    "        # aggregation of service\n",
    "        df_scoms = pd.read_csv('../bin/monthly/df_scoms_'+str(year).zfill(4)+str(month).zfill(2)+'.csv')\n",
    "        df_scoms = df_scoms.merge(tel_n_acc,how='inner',on='TEL').drop('TEL',axis=1)\n",
    "        df_scoms['CMPTIME'] = pd.to_timedelta(df_scoms.CMPTIME)\n",
    "        df_scoms = df_scoms.loc[df_scoms.elapse>60]\n",
    "        \n",
    "        # adjust elapse date\n",
    "        df_scoms['elapse'] = df_scoms['elapse'].apply(lambda x:x-60 if x>60 else x)\n",
    "        \n",
    "        df_scoms_agg_detail = df_scoms.groupby('ACCOUNT').sum().drop('elapse',axis=1)\n",
    "        df_scoms_agg_recency = df_scoms.groupby('ACCOUNT').agg({'elapse':[np.max,np.min,np.median,np.mean]})\n",
    "\n",
    "        # aggregation of invoice\n",
    "        df_invs = pd.read_csv('../bin/monthly/df_invs_'+str(year).zfill(4)+str(month).zfill(2)+'.csv')\n",
    "        df_invs = df_invs.loc[df_invs.ACCOUNT.isin(acc_list)]\n",
    "        df_invs = df_invs.drop(['BILL_DT','DUE_DATE','SETTLED_DT'],axis=1)\n",
    "        df_invs['ADJUST'] = df_invs.ADJUST.fillna(0)\n",
    "\n",
    "        # filter out records that already have been counted as churn in the past (elapse > 90)\n",
    "        serie_filt =  df_invs.groupby('ACCOUNT')['elapse_due'].agg(lambda x:True if x.min() < 90 else False)\n",
    "        \n",
    "        acc_keep = serie_filt.loc[serie_filt==True].index\n",
    "        df_invs = df_invs.loc[df_invs.ACCOUNT.isin(acc_keep)]\n",
    "\n",
    "        # define churn target as account with min settled elapse > 60\n",
    "        serie_acc_churn = df_invs.groupby('ACCOUNT')['elapse_due'].agg(lambda x:1 if x.min() > 60 else 0)\n",
    "        serie_acc_churn.name = 'churn'\n",
    "\n",
    "        # filter out records with elapse > 60\n",
    "        df_invs['elapse_settled'] = df_invs['elapse_settled']-60\n",
    "        df_invs['elapse_due'] = df_invs['elapse_due']-60\n",
    "        df_invs_agg = df_invs.loc[df_invs.elapse_due > 0].groupby('ACCOUNT').agg([np.max,np.min,np.median,np.mean])\n",
    "        df_invs_agg.columns = [col + '-' + stat for col, stat in zip(df_invs_agg.columns.get_level_values(0),df_invs_agg.columns.get_level_values(1))]\n",
    "\n",
    "        # merge all feature\n",
    "        df_all = pd.concat([df_accs,\n",
    "                            df_prod_agg_act,\n",
    "                            df_prods_agg_past,\n",
    "                            df_scoms_agg_detail,\n",
    "                            df_scoms_agg_recency,\n",
    "                            df_invs_agg,serie_acc_churn],\n",
    "                            axis=1).dropna(subset=['churn']).fillna(0)\n",
    "\n",
    "        df_all.to_csv('../bin/monthly/df_feat_'+str(year).zfill(4)+str(month).zfill(2)+'.csv')\n",
    "    \n",
    "    except:\n",
    "        print(f'month: {month}, year: {year} skipped...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recheck the validity of target definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for account in acc_keep... their min elapse due should be <90 -----> valid\n",
    "np.sum(df_invs.loc[df_invs.ACCOUNT.isin(acc_keep)].groupby('ACCOUNT')['elapse_due'].agg(lambda x:x.min())>90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invs = df_invs.loc[df_invs.ACCOUNT.isin(acc_keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define churn target as account with min settled elapse > 60\n",
    "serie_acc_churn = df_invs.groupby('ACCOUNT')['elapse_due'].agg(lambda x:1 if x.min() > 60 else 0)\n",
    "serie_acc_churn.name = 'churn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21708, 224, 0.010318776487930717)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(serie_acc_churn),np.sum(serie_acc_churn), np.sum(serie_acc_churn)/len(serie_acc_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['03860012C092D86AB3096D8E3898AFE2', '04822E975F38F75D4559C104ECE9146B',\n",
       "       '05507CEBF6D1C99EB55502168C7AF142', '05AF514541AC911832A74191A99EA493',\n",
       "       '0676AA1421F1A087855CDCDC0EE0D895', '0884F8DD1FE3465F03C0437952A67F49',\n",
       "       '0AC543E2EEBB1E7CFDA1627C840109E6', '0B7DAD5CC14BBC44B4A68FF30DE486C7',\n",
       "       '0B9751082AE536B5A190867C69CF7287', '0BFE4671297C4103B08E68186719C1C8',\n",
       "       ...\n",
       "       'F79CC3E5CDC24D1DC0B55801F025AE42', 'F7F358706605567459C24574FBFE5EBE',\n",
       "       'F81AD0F09E89761F8140341AAB6A8044', 'F929AF96DAD86515E099AF5DC75850F2',\n",
       "       'F990E0556A4C1A7196C1AC4A992C9CCF', 'FA85A6841AB6BACC65547B3B423BA42D',\n",
       "       'FAC665329E5A7B210CB814A6F188E382', 'FC2C82D11A6DDDBC17C49C91072C75C4',\n",
       "       'FDAE4B20818DC1CAF08289AA8BF72212', 'FFB22E58737FF6B9A21C1DF79B496D7D'],\n",
       "      dtype='object', name='ACCOUNT', length=224)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serie_acc_churn[serie_acc_churn==1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_invs[df_invs.ACCOUNT == 'FA85A6841AB6BACC65547B3B423BA42D'].elapse_due.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
